### Overview

Here we aimed to reproduce the style transfer methods used in `Going Beyond Nouns With Vision & Language Models Using Synthetic Data` ([paper link](https://arxiv.org/pdf/2303.17590)) to reduce the domain gap introduced by their generated synthetic images. This paper uses the style transfer methods described in `Arbitrary Style Transfer in Real-time with Adaptive Instance Normalization` ([paper link](https://arxiv.org/pdf/1703.06868)). We downloaded model weights for the pre-trained AdaIN from [this repo](https://github.com/naoto0804/pytorch-AdaIN?tab=readme-ov-file) and the code for the original paper from [this repo](https://github.com/uvavision/SyViC). We tested this method on several synthetic images provided in the appendix of the original paper (content images) and two images from HMDB51 (style images) with different interpolation factors. 

We also tried to improve the style transfer results using more recent image to image models. Results for this can be seen in `flux ip` and `kolors ip` which contain the stylized versions of the original content image located in `appendix_cont`. We find that these image to image models are stronger than the style transfer methods used in the original paper but generally run the risk of changing the image too much or corrupting important compositional details. To minimize this effect, we also tried using AdaIN style transfer using the images in `flux ip` and `kolors ip` as the style image. Results for this are located in `adain_flux` and `adain_kolors`.
