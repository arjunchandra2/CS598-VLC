### Overview

Here we aimed to reproduce the style transfer methods used in `Going Beyond Nouns With Vision & Language Models Using Synthetic Data` ([paper link](https://arxiv.org/pdf/2303.17590)) to reduce the domain gap introduced by their generated synthetic images. This paper uses the style transfer methods described in `Arbitrary Style Transfer in Real-time with Adaptive Instance Normalization` ([paper link](https://arxiv.org/pdf/1703.06868)). We downloaded model weights for the pre-trained AdaIN from [this repo](https://github.com/naoto0804/pytorch-AdaIN?tab=readme-ov-file) and the code for the original paper from [this repo](https://github.com/uvavision/SyViC). We tested this method on several synthetic images provided in the appendix of the original paper (content images) and two images from HMDB51 (style images) with different interpolation factors. 
